# All the paths for the specific directories for the datasets, weights, etc.
[paths]
# Set this to a compiled version of glove (ie the build folder)
GLOVE_PATH = '/home/codexetreme/Desktop/Source/GloVe-1.2/build'

# The path to the dataset
PATH_TO_DATASET = '/home/codexetreme/Desktop/datasets/armageddon_dataset/cnn_stories_tokenized'

# Set this is to the path of the text file that contains the dataset corpus
PATH_TO_CORPUS = '/home/codexetreme/Desktop/datasets/armageddon_dataset/corpus.txt'

# This path is where the VOCAB_FILE_NAME file will be saved
PATH_TO_VOCAB_TXT = '/home/codexetreme/Desktop/datasets/armageddon_dataset/vocab.txt'

# Path to the root folder of processed pkl and .dat files for glove
PROCESSED_GLOVE_PATH = '/home/codexetreme/Desktop/datasets/armageddon_dataset'

# Path to folder for dataset's selected vocabulary
# This contains the word2id and the vectors as .dat files
PATH_TO_DATASET_S_VOCAB = '/home/codexetreme/Desktop/datasets/armageddon_dataset/vocab'

# ==================================================================================================================

# These are mostly the only parameters you will need to change, adjust them 
# accordingly
[globals]
# Name the configurations. For example, 'COCO', 'Experiment 3', ...etc.
# Useful if your code needs to do things differently depending on which
# experiment is running.
NAME = 'DQN-Extrative-Text-Summarizer'

# NUMBER OF GPUs to use. When using only a CPU, this needs to be set to 1.
GPU_COUNT = 1

# Batch Size: The number of documents that it will take per minibatch,
# Default: 64
BATCH_SIZE = 1

# Number of epochs/episodes
NUM_EPOCHS = 2

# Number of validation steps to run at the end of every training epoch.
# A bigger number improves accuracy of validation stats, but slows
# down the training.
# Default = 50
VALIDATION_STEPS = 50

# Learning rate and momentum
LEARNING_RATE = 0.001
LEARNING_MOMENTUM = 0.9

# Select the optimizer to use, here we use optimizers that dynamically change the 
# learning rate as these are the ones required in Deep Q learning 
# options ['RMSProp', 'Adam']
LEARNING_OPTIMIZER = 'RMSProp'

# Weight decay regularization
WEIGHT_DECAY = 0.0001

# Train or freeze batch normalization layers
#     None: Train BN layers. This is the normal mode
#     False: Freeze BN layers. Good when using a small batch size
#     True: (don't use). Set layer in training mode even when predicting
TRAIN_BN = False  # Defaulting to False since batch size is often small
# ================================================================================================================== 

# These options can be used to tweak the different aspects of loading the 
# dataset. Mostly, leave them at the defaults.

# WARNING: going higher will result in more memory usage.
[dataset_options]
# Total words used in the vocab list, higher the number, slower will be the training.
VOCAB_SIZE = 6500

# The maximum total number of words per sentence. There may be padding to make sure every
# sentence is of uniform 50 words length, this is defined by the USE_PADDING_FOR_SENTENCE option
# Default = 50
MAX_WORDS_PER_SENTENCE = 50

# Maximum number of sentences per document, these may be padded,
# defined by USE_PADDING_FOR_DOCUMENT
# Default = 50
MAX_SENTENCES_PER_DOCUMENT = 50

# Sets wether the sentences have to be padded or not.
# True (default): will pad sentences to MAX_WORDS_PER_SENTENCE length,
# False: no padding is applied
USE_PADDING_FOR_SENTENCE = True

# This is the minimum number of times the word should occur in the corpus to be included in the 
# vocabulary list
MIN_VOCAB_COUNT = 5

# The dimentions for the words from glove. Default = 100
WORD_DIMENTIONS = 100
# Sets wether the sentences have to be padded or not.
# True (default): will pad documents to MAX_SENTENCES_PER_DOCUMENT length,
# False : no padding is applied
USE_PADDING_FOR_DOCUMENT = True

# ==================================================================================================================

[padding_tokens]
# Strings that represent the special padding sequences.
PADDING_SEQ='<PAD>'
UNKNOWN_SEQ='<UNK>'
START_SEQ='<START>'
END_SEQ='<END>'

# ==================================================================================================================

[sentence_enc]
FILTER_SIZES = [(1,40),(2,50),(3,50),(4,60),(5,60),(6,70),(7,70)]

[document_enc]
BIDIRECTIONAL = True
HIDDEN_SIZE = 50

# ==================================================================================================================

[dqn]

# Size of replay memory that stores the transitions, (S_i, a_i, r_i,S_[i+1])
# Default = 200,000
REPLAY_MEM_SIZE = 200000
# discount factor, aka Lambda
GAMMA = 0.999 
EPS_START = 0.1
EPS_END = 0.05
EPS_DECAY = 200
TARGET_UPDATE = 10

[q_func]
USE_RELATIVE_EMBEDDING = True
POS_DIM = 50
POS_NUM = 100
SEG_NUM = 10
